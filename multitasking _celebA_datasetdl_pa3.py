# -*- coding: utf-8 -*-
"""M22MA004_DL_PA3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_RpbF4d1LkOXznOfWCn6JnI577nNScm
"""

import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import os
import matplotlib.pyplot as plt
import seaborn as sns
import torch

!unzip /content/drive/MyDrive/celeba.zip -d path_to_directory2

import torchvision.datasets as datasets
import torchvision.transforms as transforms

!cp -r /content/path_to_directory2/celeba /content/

!unzip /content/path_to_directory2/celeba/img_align_celeba.zip -d /content/path_to_directory2/celeba

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize image pixel values
])

train_dataset = datasets.CelebA(root='/content/path_to_directory2', split='train',transform=transform, download=False)

# Define the data loader
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

# Iterate over the data loader
#for images, targets in train_loader:
    # Do something with the batch of images and targets
    #print(images.shape)
    #print(targets.shape)

# Get a batch of images from the data loader
batch = next(iter(train_loader))

# Plot the images
plt.figure(figsize=(8, 8))
for i in range(4):
    plt.subplot(2, 2, i+1)
    plt.imshow(batch[0][i].permute(1, 2, 0))
    plt.axis('off')
plt.show()

import torch
import torch.nn as nn
import torchvision.models as models

# Load the ResNet18 model
resnet18 = models.resnet18(pretrained=True)

# Modify the output layer to have 8 classes
num_ftrs = resnet18.fc.in_features
resnet18.fc = nn.Linear(num_ftrs, 8)

# Print the modified ResNet18 model architecture
print(resnet18)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(resnet18.parameters(), lr=0.001)

for epoch in range(2):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data

        # Select the first 8 attributes as the target
        labels = labels[:, :8]

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = resnet18(inputs)
        loss = criterion(outputs, labels.float())
        loss.backward()
        optimizer.step()

        # Print statistics
        running_loss += loss.item()
        if i % 100 == 99:    # Print every 100 mini-batches
            print('[Epoch %d, Batch %5d] Loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 100))
            running_loss = 0.0

test_dataset = datasets.CelebA(root='/content/path_to_directory2', split='test', transform=transform, download=False)
test_loader =torch.utils.data. DataLoader(test_dataset, batch_size=64, shuffle=True)

resnet18.eval()  # Set model to evaluation mode
total_correct = 0
total_samples = 0
with torch.no_grad():  # Disable gradient computation to speed up inference
    for inputs, labels in test_loader:
        #inputs = inputs.to(device)
        #labels = labels.to(device)

        # Select the first 8 attributes as the target
        labels = labels[:, :8]

        # Forward pass
        outputs = resnet18(inputs)
        predicted = (outputs > 0.5).float()
  
        # Calculate accuracy
        total_correct += (predicted == labels).all(dim=1).sum().item()
        total_samples += labels.shape[0]

accuracy = total_correct / total_samples
print('Test accuracy: %.2f%%' % (accuracy * 100))

resnet18.eval()  # Set model to evaluation mode

# Initialize counters for each task
task_correct = [0]*8
task_total = [0]*8

with torch.no_grad():  # Disable gradient computation to speed up inference
    for inputs, labels in test_loader:
        # Select the first 8 attributes as the target
        labels = labels[:, :8]

        # Forward pass
        outputs = resnet18(inputs)
        predicted = (outputs > 0.5).float()

        # Calculate accuracy for each task separately
        for i in range(8):
            task_correct[i] += (predicted[:, i] == labels[:, i]).sum().item()
            task_total[i] += labels[:, i].numel()

# Print accuracy for each task separately
for i in range(8):
    task_accuracy = task_correct[i] / task_total[i]
    print(f"Task {i+1} accuracy: {task_accuracy*100:.2f}%")

"""second queation"""

import matplotlib.pyplot as plt
import torch.nn.functional as F
resnet18.eval()  # Set model to evaluation mode

# Initialize counters for each task
task_correct = [0]*8
task_total = [0]*8
task_loss = [0]*8
num_batches = len(test_loader)

with torch.no_grad():  # Disable gradient computation to speed up inference
    for inputs, labels in test_loader:
        # Select the first 8 attributes as the target
        labels = labels[:, :8]

        # Forward pass
        outputs = resnet18(inputs)
        predicted = (outputs > 0.5).float()

        # Calculate loss and accuracy for each task separately
        for i in range(8):
            task_correct[i] += (predicted[:, i] == labels[:, i]).sum().item()
            task_total[i] += labels[:, i].numel()
            task_loss[i] += F.binary_cross_entropy_with_logits(outputs[:, i], labels[:, i].float(), reduction='sum').item()

# Calculate task-wise accuracy and loss
task_accuracy = [task_correct[i] / task_total[i] for i in range(8)]
task_loss = [task_loss[i] / task_total[i] for i in range(8)]

# Plot the graph
plt.plot(task_loss, task_accuracy, 'bo')
plt.xlabel('Test Loss')
plt.ylabel('Task-wise Accuracy')
plt.title('Test Loss vs. Task-wise Accuracy')
plt.show()

from google.colab import drive
drive.mount('/content/drive')

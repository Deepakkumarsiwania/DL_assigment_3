# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mw4ywjOuYwXkNG8McxOi9NhjjlABjtlR
"""

import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import os
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torchvision.datasets as datasets
import torchvision.transforms as transforms

!unzip /content/drive/MyDrive/celeba.zip -d path_to_directory2

import torchvision.datasets as datasets
import torchvision.transforms as transforms

!unzip /content/path_to_directory2/celeba/img_align_celeba.zip -d /content/path_to_directory2/celeba

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize image pixel values
])

train_dataset= datasets.CelebA(root='/content/path_to_directory2', split='train',transform=transform, download=False)

#from torchvision.datasets import CelebA, AttrLoader

# Define the attributes you want to extract
attributes = ['Eyeglasses', 'Smiling', 'Male', 'No_Beard', 'Blond_Hair', 'Young', 'Pale_Skin', 'Big_Nose']

# Create a CelebA dataset object
celeba_dataset = datasets.CelebA(root='/content/path_to_directory2', split='train', download=False)

# Create an AttrLoader object to extract the desired attributes
#attr_loader = AttrLoader(celeba_dataset, attr_names=attributes)

# Loop through the dataset and extract the attributes
for i in range(len(celeba_dataset)):
    image, attributes = celeba_dataset[i]
(attributes)

# Define the attribute you want to count
attribute = 'Eyeglasses'
# Create a CelebA dataset object
celeba_dataset = datasets.CelebA(root='/content/path_to_directory2', split='train', download=False)

# Get the index of the attribute
attr_idx = celeba_dataset.attr_names.index(attribute)

# Count the number of images with the attribute
count = 0
for i in range(len(celeba_dataset)):
    if celeba_dataset[i][1][attr_idx] == 1:
        count += 1

# Print the count
print("Number of images with attribute '" + attribute + "':", count)

#from torchvision.datasets import CelebA

# Define the attributes you want to count
attributes = ['Eyeglasses','Smiling', 'Male', 'No_Beard', 'Blond_Hair', 'Young', 'Pale_Skin', 'Big_Nose']

# Create a CelebA dataset object
celeba_dataset = datasets.CelebA(root='/content/path_to_directory2', split='train', download=False)

# Get the indices of the attributes
attr_indices = [celeba_dataset.attr_names.index(attr) for attr in attributes]

# Count the number of images with each attribute
counts = [0] * len(attributes)
for i in range(len(celeba_dataset)):
    for j in range(len(attr_indices)):
        if celeba_dataset[i][1][attr_indices[j]] == 1:
            counts[j] += 1

# Print the counts for each attribute
for i in range(len(attributes)):
    print("Number of images with attribute '" + attributes[i] + "':", counts[i])

# Find the maximum count among all attributes
max_count = max(counts)

# Print the maximum count
print("Maximum number of images with any attribute:", max_count)

# Normalize the counts by the maximum count
normalized_counts = [count / max_count for count in counts]

# Print the normalized counts for each attribute
for i in range(len(attributes)):
    print("Normalized count for attribute '" + attributes[i] + "':", normalized_counts[i])

# Calculate the average of the normalized counts
avg_normalized_count = sum(normalized_counts) / len(attributes)

# Print the average normalized count
print("Average normalized count:", avg_normalized_count)

# Filter out attributes whose normalized count is less than the average
filtered_attributes = [attributes[i] for i in range(len(attributes)) if normalized_counts[i] >= avg_normalized_count]

# Print the filtered attributes
print("Filtered attributes:", filtered_attributes)

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models

# Define the batch size
batch_size = 32

# Define the transformations for the input images
transform = transforms.Compose([
    transforms.CenterCrop(178),
    transforms.Resize(128),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# Load the CelebA dataset with specific task attributes
train_dataset = datasets.CelebA(root='/content/path_to_directory2', split='train', transform=transform, download=False,)# target_type=['Smiling', 'Male', 'No_Beard', 'Young'], target_transform=[attr_list.index(attr) for attr in ['Smiling', 'Male', 'No_Beard', 'Young']])#lambda x: ['smiling'[31], 'Male'[20], 'No_Beard'[24], 'Young'[39]])
attribute_names = celeba_dataset.attr_names

print(attribute_names)
# Define the data loader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

attr_list = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']

indices = [attr_list.index(attr) for attr in ['Smiling', 'Male', 'No_Beard', 'Young']]
print(indices)

import torch
import torch.nn as nn
import torchvision.models as models

# Load the ResNet18 model
resnet18 = models.resnet18(pretrained=True)

# Modify the output layer to have 8 classes
num_ftrs = resnet18.fc.in_features
resnet18.fc = nn.Linear(num_ftrs, 4)

# Print the modified ResNet18 model architecture
print(resnet18)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(resnet18.parameters(), lr=0.001)

for epoch in range(2):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data

        # Select the first 8 attributes as the target
        labels = labels[:,[31,24,39,20]]

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = resnet18(inputs)
        loss = criterion(outputs, labels.float())
        loss.backward()
        optimizer.step()

        # Print statistics
        running_loss += loss.item()
        if i % 100 == 99:    # Print every 100 mini-batches
            print('[Epoch %d, Batch %5d] Loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 100))
            running_loss = 0.0

test_dataset = datasets.CelebA(root='/content/path_to_directory2', split='test', transform=transform, download=False)
test_loader =torch.utils.data. DataLoader(test_dataset, batch_size=64, shuffle=True)

resnet18.eval()  # Set model to evaluation mode
total_correct = 0
total_samples = 0
with torch.no_grad():  # Disable gradient computation to speed up inference
    for inputs, labels in test_loader:
        #inputs = inputs.to(device)
        #labels = labels.to(device)

        # Select the first 8 attributes as the target
        labels = labels[:,[31,24,39,20]]

        # Forward pass
        outputs = resnet18(inputs)
        predicted = (outputs > 0.5).float()
  
        # Calculate accuracy
        total_correct += (predicted == labels).all(dim=1).sum().item()
        total_samples += labels.shape[0]

accuracy = total_correct / total_samples
print('Test accuracy: %.2f%%' % (accuracy * 100))

